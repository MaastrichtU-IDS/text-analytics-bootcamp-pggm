{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### PGGM Bootcamp Text Analytics 2020\n",
    "*Notebook by [Pedro V Hernandez Serrano](https://github.com/pedrohserrano)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "![](images/2_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Text Features Construction\n",
    "* [2.2.1. Sentiment scores](#2.2.1)\n",
    "* [2.2.2. Analysis of Readability](#2.2.2)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/sraf.png)\n",
    ".\n",
    "\n",
    "[SRAF](https://sraf.nd.edu) is a website designed to provide a central repository for programs and data used in accounting and finance research.\n",
    "\n",
    "[Article](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2493941)\n",
    "More sources for negative and positive scoring [here](https://www.aclweb.org/anthology/W18-3306.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('pickle/AnnualReports_corpus.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>report</th>\n",
       "      <th>company_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ABN_AMRO_Group_(2018).pdf</th>\n",
       "      <td>babn amro bank nvabn amro group nv annual repo...</td>\n",
       "      <td>ABN_AMRO_Group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AGNC_Investment_(2018).pdf</th>\n",
       "      <td>bproviding private capital to the us housing m...</td>\n",
       "      <td>AGNC_Investment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A_G_Barr_(2018).pdf</th>\n",
       "      <td>bag barr plc nannual report nand accounts njan...</td>\n",
       "      <td>A_G_Barr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Aboitiz_Power_(2018).pdf</th>\n",
       "      <td>bscanned with  sheet nn n  n n  nn n  n n  nn ...</td>\n",
       "      <td>Aboitiz_Power</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Acer_(2018).pdf</th>\n",
       "      <td>bacer  annual reportnnpublication date  april ...</td>\n",
       "      <td>Acer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                       report  \\\n",
       "ABN_AMRO_Group_(2018).pdf   babn amro bank nvabn amro group nv annual repo...   \n",
       "AGNC_Investment_(2018).pdf  bproviding private capital to the us housing m...   \n",
       "A_G_Barr_(2018).pdf         bag barr plc nannual report nand accounts njan...   \n",
       "Aboitiz_Power_(2018).pdf    bscanned with  sheet nn n  n n  nn n  n n  nn ...   \n",
       "Acer_(2018).pdf             bacer  annual reportnnpublication date  april ...   \n",
       "\n",
       "                               company_name  \n",
       "ABN_AMRO_Group_(2018).pdf    ABN_AMRO_Group  \n",
       "AGNC_Investment_(2018).pdf  AGNC_Investment  \n",
       "A_G_Barr_(2018).pdf                A_G_Barr  \n",
       "Aboitiz_Power_(2018).pdf      Aboitiz_Power  \n",
       "Acer_(2018).pdf                        Acer  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = data.report[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2.2.1. Sentiment scores\n",
    "<a id=\"2.2.1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read dictionary function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dictionary(path):\n",
    "    file = open(path,'r')\n",
    "    return file.read().lower().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading positive words\n",
    "positive_words = read_dictionary('dictionaries/positive_words.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#positive_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading negative words\n",
    "negative_words = read_dictionary('dictionaries/negative_words.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#negative_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a function that counts the number of times a positive or negative words appear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Score generator function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate scores\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def generate_score(text, list_to_compare):\n",
    "    numWords = 0\n",
    "    tokens = word_tokenize(text)\n",
    "    for word in tokens:\n",
    "        if word in list_to_compare:\n",
    "            numWords  += 1\n",
    "    \n",
    "    cumsum = numWords\n",
    "    return cumsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "834"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating positive score \n",
    "positive_score = generate_score(report, positive_words)\n",
    "positive_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1401"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating negative score \n",
    "negative_score = generate_score(report, negative_words)\n",
    "negative_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating polarity score\n",
    "def polarity_score(positive_score, negative_score):\n",
    "    pol_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.00001)\n",
    "    return pol_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.2536912740327012"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polarity_score(positive_score, negative_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corpus lenght"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting total words\n",
    "def total_word_count(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106119"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens = total_word_count(report)\n",
    "count_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading uncertainty dictionary\n",
    "uncertainty_dict = read_dictionary('dictionaries/uncertainty_dictionary.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1728"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculating uncertainty_score\n",
    "uncertainty_score = generate_score(report, uncertainty_dict)\n",
    "uncertainty_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading constraining words\n",
    "constraining_dict = read_dictionary('dictionaries/constraining_dictionary.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "686"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculating constraining score\n",
    "constraining_score = generate_score(report, constraining_dict)\n",
    "constraining_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2.2.3. Analysis of Readability\n",
    "<a id=\"2.2.3\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating Average sentence length \n",
    "# It will calculated using formula --- Average Sentence Length = the number of words / the number of sentences\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize     \n",
    "    \n",
    "def average_sentence_length(text):\n",
    "    sentence_list = sent_tokenize(text)\n",
    "    tokens = word_tokenize(text)\n",
    "    average_sent_length = len(tokens) / len(sentence_list) + 0.000001\n",
    "    return round(average_sent_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106119"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_sentence_length = average_sentence_length(report)\n",
    "avg_sentence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating percentage of complex word \n",
    "# It is calculated using Percentage of Complex words = the number of complex words / the number of words \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def percentage_complex_word(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    complexWord = 0\n",
    "    complex_word_percentage = 0\n",
    "    \n",
    "    for word in tokens:\n",
    "        vowels=0\n",
    "        if word.endswith(('es','ed')):\n",
    "            pass\n",
    "        else:\n",
    "            for w in word:\n",
    "                if(w=='a' or w=='e' or w=='i' or w=='o' or w=='u'):\n",
    "                    vowels += 1\n",
    "            if(vowels > 2):\n",
    "                complexWord += 1\n",
    "    if len(tokens) != 0:\n",
    "        complex_word_percentage = complexWord/len(tokens)\n",
    "    \n",
    "    return complex_word_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3049312564196798"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perc_complex_word = percentage_complex_word(report)\n",
    "perc_complex_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating Fog Index \n",
    "# Fog index is calculated using -- Fog Index = 0.4 * (Average Sentence Length + Percentage of Complex words)\n",
    "def calculate_fog_index(averageSentenceLength, percentageComplexWord):\n",
    "    fogIndex = 0.4 * (averageSentenceLength + percentageComplexWord)\n",
    "    return round(fogIndex,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42447.722"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fog_index = calculate_fog_index(avg_sentence_length, perc_complex_word)\n",
    "fog_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### *There are loads of sources related to Analysis of Readability, some examples:*\n",
    "- Readability Index at [geeksforgeeks.org](https://www.geeksforgeeks.org/readability-index-pythonnlp/) and [geeksforgeeks.org](https://www.tutorialspoint.com/readability-index-in-python-nlp)\n",
    "- Automated readability assessment [Wikipedia](https://en.wikipedia.org/wiki/Readability#The_Golub_Syntactic_Density_Score) and [Article](https://www.aclweb.org/anthology/R15-1014.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only thing missing is to integrate the new information applying the functions created to the whole corpus and therefore augment the knowledge of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 351.0015389919281 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "#--- 351.0015389919281 seconds ---\n",
    "\n",
    "data['count_tokens'] = data.report.apply(total_word_count)\n",
    "data['average_sentence_length'] = data.report.apply(average_sentence_length)\n",
    "data['percentage_complex_word'] = data.report.apply(percentage_complex_word)\n",
    "data['positive_score'] = data.report.apply(lambda x: generate_score(x, positive_words))\n",
    "data['negative_score'] = data.report.apply(lambda x: generate_score(x, negative_words))\n",
    "data['uncertainty_score'] = data.report.apply(lambda x: generate_score(x, uncertainty_dict))\n",
    "data['constraining_score'] = data.report.apply(lambda x: generate_score(x, constraining_dict))\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(92, 9)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Dataset with text features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>report</th>\n",
       "      <th>company_name</th>\n",
       "      <th>count_tokens</th>\n",
       "      <th>average_sentence_length</th>\n",
       "      <th>percentage_complex_word</th>\n",
       "      <th>positive_score</th>\n",
       "      <th>negative_score</th>\n",
       "      <th>uncertainty_score</th>\n",
       "      <th>constraining_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ABN_AMRO_Group_(2018).pdf</th>\n",
       "      <td>babn amro bank nvabn amro group nv annual repo...</td>\n",
       "      <td>ABN_AMRO_Group</td>\n",
       "      <td>106119</td>\n",
       "      <td>106119</td>\n",
       "      <td>0.304931</td>\n",
       "      <td>834</td>\n",
       "      <td>1401</td>\n",
       "      <td>1728</td>\n",
       "      <td>686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AGNC_Investment_(2018).pdf</th>\n",
       "      <td>bproviding private capital to the us housing m...</td>\n",
       "      <td>AGNC_Investment</td>\n",
       "      <td>47122</td>\n",
       "      <td>47122</td>\n",
       "      <td>0.301091</td>\n",
       "      <td>297</td>\n",
       "      <td>858</td>\n",
       "      <td>963</td>\n",
       "      <td>412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A_G_Barr_(2018).pdf</th>\n",
       "      <td>bag barr plc nannual report nand accounts njan...</td>\n",
       "      <td>A_G_Barr</td>\n",
       "      <td>54311</td>\n",
       "      <td>54311</td>\n",
       "      <td>0.272947</td>\n",
       "      <td>662</td>\n",
       "      <td>461</td>\n",
       "      <td>457</td>\n",
       "      <td>230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Aboitiz_Power_(2018).pdf</th>\n",
       "      <td>bscanned with  sheet nn n  n n  nn n  n n  nn ...</td>\n",
       "      <td>Aboitiz_Power</td>\n",
       "      <td>68940</td>\n",
       "      <td>68940</td>\n",
       "      <td>0.261053</td>\n",
       "      <td>262</td>\n",
       "      <td>529</td>\n",
       "      <td>425</td>\n",
       "      <td>224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Acer_(2018).pdf</th>\n",
       "      <td>bacer  annual reportnnpublication date  april ...</td>\n",
       "      <td>Acer</td>\n",
       "      <td>89820</td>\n",
       "      <td>89820</td>\n",
       "      <td>0.327533</td>\n",
       "      <td>684</td>\n",
       "      <td>1134</td>\n",
       "      <td>572</td>\n",
       "      <td>443</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                       report  \\\n",
       "ABN_AMRO_Group_(2018).pdf   babn amro bank nvabn amro group nv annual repo...   \n",
       "AGNC_Investment_(2018).pdf  bproviding private capital to the us housing m...   \n",
       "A_G_Barr_(2018).pdf         bag barr plc nannual report nand accounts njan...   \n",
       "Aboitiz_Power_(2018).pdf    bscanned with  sheet nn n  n n  nn n  n n  nn ...   \n",
       "Acer_(2018).pdf             bacer  annual reportnnpublication date  april ...   \n",
       "\n",
       "                               company_name  count_tokens  \\\n",
       "ABN_AMRO_Group_(2018).pdf    ABN_AMRO_Group        106119   \n",
       "AGNC_Investment_(2018).pdf  AGNC_Investment         47122   \n",
       "A_G_Barr_(2018).pdf                A_G_Barr         54311   \n",
       "Aboitiz_Power_(2018).pdf      Aboitiz_Power         68940   \n",
       "Acer_(2018).pdf                        Acer         89820   \n",
       "\n",
       "                            average_sentence_length  percentage_complex_word  \\\n",
       "ABN_AMRO_Group_(2018).pdf                    106119                 0.304931   \n",
       "AGNC_Investment_(2018).pdf                    47122                 0.301091   \n",
       "A_G_Barr_(2018).pdf                           54311                 0.272947   \n",
       "Aboitiz_Power_(2018).pdf                      68940                 0.261053   \n",
       "Acer_(2018).pdf                               89820                 0.327533   \n",
       "\n",
       "                            positive_score  negative_score  uncertainty_score  \\\n",
       "ABN_AMRO_Group_(2018).pdf              834            1401               1728   \n",
       "AGNC_Investment_(2018).pdf             297             858                963   \n",
       "A_G_Barr_(2018).pdf                    662             461                457   \n",
       "Aboitiz_Power_(2018).pdf               262             529                425   \n",
       "Acer_(2018).pdf                        684            1134                572   \n",
       "\n",
       "                            constraining_score  \n",
       "ABN_AMRO_Group_(2018).pdf                  686  \n",
       "AGNC_Investment_(2018).pdf                 412  \n",
       "A_G_Barr_(2018).pdf                        230  \n",
       "Aboitiz_Power_(2018).pdf                   224  \n",
       "Acer_(2018).pdf                            443  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing to csv file\n",
    "output = data.drop(columns=['report'])\n",
    "#output.to_csv('datasets/table_text_features.csv', sep=',', encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nteract": {
   "version": "0.15.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
