{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### PGGM Bootcamp Text Analytics 2020\n",
    "*Notebook by [Pedro V Hernandez Serrano](https://github.com/pedrohserrano)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "![](images/1_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4 Data Analysis\n",
    "* [1.4.1. Most common words](#1.4.1)\n",
    "* [1.4.2. Wordclouds](#1.4.2)\n",
    "* [1.4.3. Relevant words analysis](#1.4.3)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory data analysis of annual reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the data cleaning step where we put our data into a few standard formats, the next step is to take a look at the data and see if what we're looking at makes sense. Before applying any algorithm, let's explore the data first.\n",
    "\n",
    "When working with numerical data, some of the exploratory data analysis (EDA) techniques we can use include finding the average of the data set, the distribution of the data, the most common values, etc. The idea is the same when working with text data. We are going to find some more obvious patterns with EDA before identifying the hidden patterns with machines learning (ML) techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Most common words** - find these and create word clouds\n",
    "2. **Size of vocabulary** - look number of unique words and total\n",
    "3. **Trade-off Risk** - most common terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1.4.1. Most Common Words\n",
    "<a id=\"1.4.1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will read the document-term matrix and explore the words per document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_matrix = pd.read_pickle('pickle/AnnualReports_matrix.pkl')\n",
    "data_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_matrix.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Find the top 30 words per report where each report is on every column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_dict = {}\n",
    "for c in data.columns:\n",
    "    top = data[c].sort_values(ascending=False).head(30)\n",
    "    top_dict[c]= list(zip(top.index, top.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = pd.DataFrame.from_dict(top_dict)\n",
    "top_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "We can try to find the number of unique words that each company uses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of companies we're working on\n",
    "companies = list(data_matrix.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the non-zero items in the document-term matrix, meaning that the word occurs at least once\n",
    "unique_list = []\n",
    "for company in data.columns:\n",
    "    uniques = data[company].to_numpy().nonzero()[0].size\n",
    "    unique_list.append(uniques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe that contains this unique word count\n",
    "data_words = pd.DataFrame(list(zip(companies, unique_list)), columns=['company', 'unique_words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the total number of words per document\n",
    "total_list = []\n",
    "for company in data.columns:\n",
    "    totals = sum(data[company])\n",
    "    total_list.append(totals)\n",
    "# Let's add some columns to our dataframe\n",
    "data_words['total_words'] = total_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having the total words we can make the calculation on relative numbers, since is expected that the lenght of unique words will increase with the lenght of the document so we need to normalize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the ratio\n",
    "data_words['unique_relative'] = data_words['unique_words']/data_words['total_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort it out\n",
    "data_sort = data_words.sort_values(by='unique_relative', ascending=False)\n",
    "data_sort.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pos = np.arange(len(data_words))\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [12, 18]\n",
    "plt.barh(y_pos, data_sort.unique_relative, align='center')\n",
    "plt.yticks(y_pos, data_sort.company)\n",
    "plt.title('Unique Words %', fontsize=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1.4.2. Wordclouds\n",
    "<a id=\"1.4.2\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Word Cloud is a good option to help visually interpret text at first gaze and gain insight into the most prominent items in a given text, by visualizing the word frequency in the text as a weighted list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wordcloud python library\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dfining wordcloud object\n",
    "wc = WordCloud(background_color=\"white\", colormap=\"Dark2\", max_font_size=150, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start to plot we would first need our corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the corpus in dataframe\n",
    "data_clean = pd.read_pickle('pickle/AnnualReports_corpus.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 'Pfizer_(2018).pdf' #'ABN_AMRO_Group_(2018).pdf' \n",
    "wc.generate(data_clean.report[index])\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(index)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### *Learn more about Wordcloud library on its official documentation [amueller.github.io/word_cloud](https://amueller.github.io/word_cloud/index.html)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1.4.3. Relevant words analysis (Risk-loss trade-off)\n",
    "<a id=\"1.4.3\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among our corpus we might find relevant certain terms related to something in particular, like negative or positive sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's isolate just these risk words\n",
    "data_risk_words = data_matrix[['risk', 'loss']]\n",
    "data_risk_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We include the total count to this new dataframe\n",
    "data_risk_words['total_words'] = list(data_sort.sort_values('company').total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We calculate the relatives\n",
    "data_risk_words['relative_risk'] = data_risk_words['risk']/data_risk_words['total_words']*1000\n",
    "data_risk_words['relative_loss'] = data_risk_words['loss']/data_risk_words['total_words']*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_risk_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> \n",
    "We simply scatterplot the word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [16, 16]\n",
    "\n",
    "for i, company in enumerate(data_risk_words.index):\n",
    "    x = round(data_risk_words.relative_risk.loc[company],2)\n",
    "    y = round(data_risk_words.relative_loss.loc[company],2)\n",
    "    plt.scatter(x, y, color='blue')\n",
    "    plt.text(x+0.1, y+0.1, data_risk_words.index[i], fontsize=8)\n",
    "    \n",
    "plt.title('Number of Risk-Loss Terms Used in Document per Thousand Words', fontsize=20)\n",
    "plt.xlabel('Risk mentions', fontsize=15)\n",
    "plt.ylabel('Loss mentions', fontsize=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### *More tips and tricks of Matplotlib python library at [realpython.com](https://realpython.com/python-matplotlib-guide/)*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nteract": {
   "version": "0.15.0"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
