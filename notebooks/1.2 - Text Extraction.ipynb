{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### PGGM Bootcamp Text Analytics 2020\n",
        "*Notebook by [Pedro V Hernandez Serrano](https://github.com/pedrohserrano)*"
      ],
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "![](images/1_2.png)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.2 Text Extraction:\n",
        "* [1.2.1. Exploring text data sources](#1.21)\n",
        "* [1.2.2. Internet parsing using API](#1.2.2)\n",
        "* [1.2.3. Internet parsing using BeautifulSoup](#1.2.3)\n",
        "\n",
        "---"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Financial Text Data Sources "
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Examples of Data Sources\n",
        "Corporate Reports  \n",
        "• SEC’s EDGAR: 1994-2015, 15+ million filings, annual and quarterly reports  \n",
        "• Regulatory disclosures: annual and interim filings (10-K and 10-Q), correspondences, IPO registration statements, etc.  \n",
        "• Company Announcements: Websites, SEC 8-K filings, news feeds  \n",
        "\n",
        "News: Newswires, articles, blogs.  \n",
        "• WSJ News Archive: XML encapsulated, 2000 - present  \n",
        "• SEC’s Current Report filings (8-K): any material new information  \n",
        "• Audio transcripts of conference calls by top executives, earnings announcements..  \n",
        "• Analyst ratings and research reports, investor surveys, investors’ activism campaigns  \n",
        "\n",
        "Social Media, other sources    \n",
        "• Twitter, Stocktwits, message boards, websites  \n",
        "• Google searches (Google Analytics)  \n",
        "• Patent Applications: [patft.uspto.gov](http://patft.uspto.gov/netahtml/PTO/search-adv.htm) and [patents.google.com](https://patents.google.com/)\n",
        "\n",
        "<br>"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "*For the purpose of the **Use case** we will use [EDGAR Company Filings](https://www.sec.gov/edgar/searchedgar/companysearch.html) and [Annual reports](http://bit.ly/39VuFHW) provided by PGGM*"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### 1.2.2. Internet Parsing using API\n",
        "<a id=\"1.2.2\">"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Before we get deep into the EDGAR data we actually need to map the CIK codes with the company names so that we can automatically can gather information\n",
        "- A Central Index Key or CIK number is a number given to an individual, company, or foreign government by the United States Securities and Exchange Commission. \n",
        "- The number will help is to identify its filings in several online databases, including EDGAR.\n",
        "- The numbers are ten digits in length."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "**There are a couple of data sources that provide the information of companies names, ticker, CIK and Exchange schema like [rankandfiled.com](http://rankandfiled.com/#/data/tickers) or [dan.vonkohorn.com](https://dan.vonkohorn.com/2016/07/03/cik-ticker-mappings/)**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#reading the ticker file\n",
        "cik_ticker = pd.read_csv('datasets/cik_ticker.csv', sep='|')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "cik_ticker.head()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "cik_ticker.Exchange.unique()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "len(cik_ticker)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accessing CIK - Ticker mappings via API"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "NASDAQ = pd.read_json('https://mapping-api.herokuapp.com/exchange/NASDAQ')\n",
        "OTC = pd.read_json('https://mapping-api.herokuapp.com/exchange/OTC')\n",
        "NYSE = pd.read_json('https://mapping-api.herokuapp.com/exchange/NYSE')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame()\n",
        "df = df.append(NASDAQ)\n",
        "df = df.append(OTC)\n",
        "df = df.append(NYSE)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# sample for example\n",
        "df = df.sample(120, random_state=1298, replace=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.head(10) #Only taking the first 10 for the moment to avoid heavy processing\n",
        "len(df)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "#### *Learn more about API at [dataquest.io](https://www.dataquest.io/blog/python-api-tutorial/) and [freecodecamp.org](https://www.freecodecamp.org/news/what-is-an-api-in-english-please-b880a3214a82/)*"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### 1.2.3. Internet Parsing using BeautifulSoup\n",
        "<a id=\"1.2.3\">"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main idea is to automate the search of companies and extract the information of the annual filings since there is no proper data source of annual reports we can always \"scrape EDGAR\" using BeautifulSoup"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "EDGAR text analysis sources:\n",
        "- [SEC Edgar Crawler](https://github.com/coyo8/sec-edgar)\n",
        "- [OpenEDGAR by LexPredict](https://github.com/LexPredict/openedgar) and [Paper](https://arxiv.org/pdf/1806.04973.pdf)     "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#main libraries for internet parsing\n",
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import time\n",
        "from datetime import datetime, timedelta"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's go step by step into EDGAR database"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. In the main page we insert a ticker to look for `AAL` for example\n",
        "2. Check at the URL, it has a query sintax `https://www.sec.gov/cgi-bin/browse-edgar?CIK=AAL`\n",
        "3. We have the annual reports listed, ordered by the date added, the last year should be the first one"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# query search\n",
        "cik = 'AAL'\n",
        "query_search = 'https://www.sec.gov/cgi-bin/browse-edgar?CIK='+str(cik)+'&type=10-K'"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# internet request\n",
        "page = requests.get(query_search)\n",
        "page"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# parsing the page\n",
        "page_parsed = BeautifulSoup(page.text, 'html.parser')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# looking for the right doc\n",
        "results_table = page_parsed.find(class_='tableFile2')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make the procedure modular"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#list of cik codes\n",
        "cik_codes = list(df.cik.unique())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# function to get the url to the file\n",
        "def get_url(query):\n",
        "    page = requests.get(query)\n",
        "    page_parsed = BeautifulSoup(page.text, 'html.parser')\n",
        "    results_table = page_parsed.find(class_='tableFile2')\n",
        "    return results_table.find_all('a')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# function to get the txt files\n",
        "def get_txt_files(cik_codes):\n",
        "    # INPUT = list of CIK codes\n",
        "    docs_urls = []\n",
        "    for cik in cik_codes:\n",
        "        query = 'https://www.sec.gov/cgi-bin/browse-edgar?CIK='+str(cik)+'&type=10-K'\n",
        "        results_files = get_url(query)\n",
        "        #if there is no files in the registry means the table and list are empty\n",
        "        if results_files != []:\n",
        "            #print(str(query))\n",
        "            results_items = results_files[0].prettify()  #here access the company\n",
        "            raw_url = 'https://www.sec.gov'+re.findall(r'\\\"(.+?)\\\"',results_items)[0]\n",
        "            url = raw_url.replace('-index.htm','.txt')\n",
        "            docs_urls.append(url)\n",
        "        else:\n",
        "            docs_urls.append('https://www.google.com/')#'no-url')\n",
        "    return docs_urls"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Web crawler to extract the URL of the text files from EDGAR database"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.monotonic()\n",
        "#execution\n",
        "urls_files = get_txt_files(cik_codes)\n",
        "#time\n",
        "sttime = datetime.now().strftime('%Y%m%d_%H:%M_')\n",
        "end_time = time.monotonic()\n",
        "ex_time = timedelta(seconds=end_time - start_time)\n",
        "print(\"Execution time: {}\".format(ex_time))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can define a new column, and add the URL of each company related to each CIK"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Only the head to test\n",
        "df = df.head(len(urls_files))\n",
        "df['file'] = urls_files"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# save to csv\n",
        "df.to_csv('datasets/table_companies_short.csv')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "tickers = list(df['ticker'])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# check the first URL\n",
        "urls_files[0]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "**WARNING: Here we actually request documents (it takes some time to run)**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def url_to_transcript(url):\n",
        "    return requests.get(url).text"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "reports = [url_to_transcript(u) for u in urls_files]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "Parsing the collected files and pickle-ing them to drive"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a new directory to hold the text files\n",
        "#!mkdir reports"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# for i, c in enumerate(tickers):\n",
        "#     with open(\"reports/\" + c + \".txt\", \"wb\") as file:\n",
        "#         pickle.dump(reports[i], file)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look of a file\n",
        "print(reports[0][:800])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        ". \n",
        "\n",
        "**Congratulations we have scraped EDGAR succesfully, to find out wether if the files are saved correctly we simply load them and see (good practice)**|"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "#### *Learn more about BeautifulSoup at [dataquest.io](https://www.dataquest.io/blog/web-scraping-beautifulsoup/ ) and [BeautifulSoup official documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/), the implementation is inspired from [scrapsfromtheloft.com](scrapsfromtheloft.com)*"
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "nteract": {
      "version": "0.15.0"
    },
    "toc": {
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}