{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### PGGM Bootcamp Text Analytics 2020\n",
    "*Notebook by [Pedro V Hernandez Serrano](https://github.com/pedrohserrano)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "![](images/2_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 NLP Tasks with Python NLTK\n",
    "* [2.1.1. Exploring NLTK](#2.1.1)\n",
    "* [2.1.2. Exploring SpaCy](#2.1.2)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Natural Language Processing\n",
    "\n",
    "- Natural Language:  \n",
    "Any language used in everyday communication by humans written or spoken  \n",
    "\n",
    "- Other Languages:   \n",
    "Constructed languages, or Computer languages  \n",
    "\n",
    "- Natural Language Processing:  \n",
    "Any computation or manipulation of natural language to get insights about how words mean and how sentences are constructed.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/nltkbook.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pro TIP: explore the functions/properties of an object\n",
    "def functions(obj):\n",
    "    return [prop for prop in dir(obj) if not prop.startswith('_')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### *Check the NLP with Python book [online version](https://www.nltk.org/book/)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the nltk package\n",
    "import nltk\n",
    "#call the nltk downloader\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The library includes already curated corpora for research proposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has loads of in-build functions to deal with text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Frequency of words\n",
    "dist = FreqDist(text7)\n",
    "len(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab1 = dist.keys()\n",
    "# In Python 3 dict.keys() returns an iterable view instead of a list\n",
    "list(vocab1)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist['risk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqwords = [w for w in vocab1 if len(w) > 5 and dist[w] > 100]\n",
    "freqwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2.1.1. Exploring NLTK\n",
    "<a id=\"2.1.1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Tokenization — convert sentences to words  \n",
    "3. Removing stop words — frequent words such as ”the”, ”is”, etc. that do not have specific semantic  \n",
    "4. Stemming — words are reduced to a root by removing inflection through dropping unnecessary characters, usually a suffix.  \n",
    "5. Lemmatization — Another approach to remove inflection by determining the part of speech and utilizing detailed database of the language.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### *Learn more about NLP steps at [towardsdatascience.com](https://towardsdatascience.com/machine-learning-text-processing-1d5a2d638958)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization NLTK\n",
    "- Is the process of converting text into tokens before transforming it into vectors. \n",
    "- It is also easier to filter out unnecessary tokens. \n",
    "- Can be document into paragraphs or sentences into words.\n",
    "\n",
    "Some types of tokenization [text-processing.com](https://text-processing.com/demo/tokenize/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean = pd.read_pickle('pickle/AnnualReports_corpus.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = data_clean.report[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to split text into word\n",
    "tokens = word_tokenize(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of sentence tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ = \"This is the first sentence. The stocks of AAL are higher. Is this the third sentence? Yes, it is!\"\n",
    "sentences = nltk.sent_tokenize(text_)\n",
    "print(len(sentences), sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopwords NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main idea is to manually manipulate the list of stop words at our will, combining the list of financial stop words created and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the stopwords method from `nltk` we can easily enlist a predefined list of English stop words for instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, using `pickle` we load the previous stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwordsfile = \"pickle/AnnualReports_stopwords.pkl\"\n",
    "file = open(stopwordsfile, \"rb\")\n",
    "financial_stop_words = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = []\n",
    "stopwords.extend(english_stop_words)\n",
    "stopwords.extend(financial_stop_words)\n",
    "stopwords = list(set(stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopWords(text, stopwords_list):\n",
    "    text = text.lower()\n",
    "    for item in stopwords_list:\n",
    "        text = text.replace(\" \" + item.lower() + \" \",\" \")\n",
    "        text = text.replace(\" \" + item.lower() + \",\",\" \")\n",
    "        text = text.replace(\" \" + item.lower() + \".\",\" \")\n",
    "        text = text.replace(\" \" + item.lower() + \";\",\" \")\n",
    "    text = text.replace(\"+\",\"\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_without_stopwords = removeStopWords(report,stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_without_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK provides several stemmer interfaces like Porter stemmer, Lancaster Stemmer, Snowball Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plurals = ['List', 'listed', 'lists', 'listing','listings']\n",
    "singles = [porter.stem(plural).lower() for plural in plurals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "singles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"assets :\", lemmatizer.lemmatize(\"assets\")) \n",
    "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\")) \n",
    "print(\"better :\", lemmatizer.lemmatize(\"better\", pos =\"a\")) # a denotes adjective in \"pos\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### *Stem sub-package from NLTK [www.nltk.org](http://www.nltk.org/api/nltk.stem.html)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2.1.2. Exploring SpaCy\n",
    "<a id=\"2.1.2\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DsDhJiCFMbx9"
   },
   "outputs": [],
   "source": [
    "# load core elements\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fh4hxAj6ZCqE"
   },
   "outputs": [],
   "source": [
    "report_nlp = nlp(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "UuhHxJjcpfWA",
    "outputId": "3ed2539c-34b8-47b1-d992-cc7824167621"
   },
   "outputs": [],
   "source": [
    "type(report_nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions(report_nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oyDgxaDfKft7"
   },
   "source": [
    "SpaCy processes everything all at once, which explains why the command above takes so long. It's doing named entity recognition, looking up word vectors, doing POS tagging, and performing other tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report lenght\n",
    "len(report_nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "BvsgGm6WpQg2",
    "outputId": "cfb9eff9-0484-48ab-c502-65d80dcecc8d"
   },
   "outputs": [],
   "source": [
    "# sentences lenght\n",
    "type(report_nlp.sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q4QRc35PuCES"
   },
   "source": [
    "_Side note: generators_\n",
    "\n",
    "Generators are functions that behave as iterators, i.e., you can iterate over them with a `for` loop, like you would with a list. But you can't index them. So this works: \n",
    "\n",
    "```python\n",
    "for sent in grailDoc.sents: \n",
    "  print(sent)\n",
    "```\n",
    "\n",
    "But not this: \n",
    "\n",
    "```python\n",
    "print(grailDoc.sents[0]) # Doesn't work\n",
    "```\n",
    "\n",
    "However, you can force a generator into a list, using `list()`, and then index it. So to get the first sentence of text, one could write:\n",
    "\n",
    "```python\n",
    "list(pride.sents)[0]\n",
    "```\n",
    "\n",
    "But actually if we just want the first one, we can do this: \n",
    "\n",
    "```python\n",
    "next(pride.sents)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6zNgVuSaEM4s",
    "outputId": "8a1cbcc5-760e-4bdb-b2ee-e02456179294"
   },
   "outputs": [],
   "source": [
    "len(list(report_nlp.sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ownHtIc6Eadq"
   },
   "source": [
    "What's the longest sentence in the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "-Sag9j9Zpigk",
    "outputId": "12590fcd-e759-4cce-a22d-0a98be4afb29"
   },
   "outputs": [],
   "source": [
    "sent_lengths = [len(sent) for sent in report_nlp.sents]\n",
    "[sent for sent in report_nlp.sents if len(sent) == max(sent_lengths)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nxwM7D4IK3RL"
   },
   "source": [
    "#### Exploring properties of the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "iDIWPQyrJteR",
    "outputId": "35063005-d139-40de-eecb-1f0feb421600"
   },
   "outputs": [],
   "source": [
    "amro = report_nlp[4]\n",
    "print(amro, type(amro))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "XsycKBCMKdTI",
    "outputId": "ba03aac5-72e3-41fb-db77-6fb179bfefbe"
   },
   "outputs": [],
   "source": [
    "amro.i, amro.idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "R6Y2E3seim6w",
    "outputId": "2562e24b-d288-464b-d9bd-8dcf132bc71a"
   },
   "outputs": [],
   "source": [
    "amro.prefix_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 395
    },
    "colab_type": "code",
    "id": "4913OKHSPonC",
    "outputId": "59ba51ac-6a18-4d31-cdc0-30a7df97e45a"
   },
   "outputs": [],
   "source": [
    "pd.Series([word.i for word in report_nlp if word.text == 'loss']).hist(figsize=(12,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nxwM7D4IK3RL"
   },
   "source": [
    "#### Exploring named entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following list shows the entities encountered in the current corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "q_t42ZAMlwRK",
    "outputId": "46bab034-b5db-4e9b-e15c-df3293aa529f"
   },
   "outputs": [],
   "source": [
    "set([w.label_ for w in report_nlp.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "id": "IGWWzw0Cxnyr",
    "outputId": "842a5a70-4e88-4494-a422-acf6c5dbf73d"
   },
   "outputs": [],
   "source": [
    "entity_sentences = [ent.sent for ent in report_nlp.ents if ent.label_ == 'MONEY']\n",
    "entity_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "Te5HAuSq819k",
    "outputId": "4e8c8eaa-999a-4f3a-c86f-66074d077d4f"
   },
   "outputs": [],
   "source": [
    "spacy.displacy.render(entity_sentences, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### POS tagging (Parts of speech)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tnv8EFnhNFDp"
   },
   "source": [
    "Get the first 100 nouns in the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "KmOYJoGaNIQt",
    "outputId": "15ec033f-6a53-45df-ad0a-d8af04a2aa15"
   },
   "outputs": [],
   "source": [
    "print([w for w in report_nlp if w.pos_ == 'NOUN'][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yebXQk35Q-in"
   },
   "source": [
    "<br>\n",
    "Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 703
    },
    "colab_type": "code",
    "id": "Y7c_T7Ovxu7R",
    "outputId": "6a3ac243-c56d-4a29-9466-601cdf326b63"
   },
   "outputs": [],
   "source": [
    "#spacy.displacy.render(entity_sentences, jupyter=True, options={'compact': True, 'collapse_punct': True, 'collapse_phrases': True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### *SpaCy one of the most popular tools for NLP nowadays [https://spacy.io](https://spacy.io)*"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "spacy.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nteract": {
   "version": "0.15.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
