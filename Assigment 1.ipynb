{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assigment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text to organized data: Annual Reports \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this assignment is to grasp the fundamentals of working with text in python performing a hands-on data pipeline where the initial input is simply the **Annual Reports** in pdf format, the idea is to reproduce what is revised in the notebooks 1.1, 1.2 and 1.3 aiming to have workable datasets which will be important building blocks for the final use case.\n",
    "\n",
    "The following assignment is divided into 3 parts: Mining PDFs, Retrieving files and Text organization where is each part will be asked to create functions or routines for data processing.  \n",
    "\n",
    "The expected output of this notebook is a clean, organized data in two standard text formats:\n",
    "\n",
    "1. **Corpus** - a collection of text\n",
    "2. **Document-Term Matrix** - word counts in matrix format\n",
    "\n",
    "Additionally a Countvectorizer object in pickle format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Mining PDFs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Intsall the library **`textract`** using pip or conda and then import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Using the library **`os`** we can access the path of our working directories, here is an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  \n",
    "opath = os.getcwd()  \n",
    "print (\"The current working directory is %s\" % opath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following function to show the files contained in that folder  \n",
    "```python\n",
    "os.listdir(opath)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Write a function or process that will create a list of names of files of certain folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Write a function that will iterate each of the names of the previous list, extract the text and write to txt format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_pdf():\n",
    "# your function\n",
    "        file.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Test your function with the pdfs contained in the folder **AnnualReportsTest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write_pdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Retrieving files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of retrieve the filenames of certain format is with via terminal `!ls path*.txt` [more](https://jakevdp.github.io/PythonDataScienceHandbook/01.05-ipython-and-shell-commands.html)\n",
    "1. Create a list with the name of the reports saved in txt, the list should be automatically generated accessing the folder      \n",
    "The list should look like this:\n",
    "```python\n",
    "['ABN_AMRO_Group_(2018).pdf', 'AGNC_Investment_(2018).pdf']...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create a function or process that will load all the existing reports in `txt` format and save it in a dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Text organization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import `pandas` and perform the tasks of data cleaning revised in the `Text Processing` notebook in order to get a Corpus and a Document-term Matrix, add or remove cleaning functions if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Financial stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The matrix and the corpus should be available in order to perform this step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Write a process or function that will retrieve the top 30 used words per document and save it in the format of your preference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "The main idea here is to filter or exclude such words from the financial jargon, wich are highly present on each document and presumibly do not represent any insigh, those could be added to a stop words list, like for instance \"financial\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Aggregate the top words that appear in all the documents and calculate the frecuency of each word among all corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `Counter` can be useful in case the top words are handled in lists or arrays  \n",
    "*more about [Counter](https://pymotw.com/2/collections/counter.html) library*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".\n",
    "\n",
    "**Note: The creation of stop words lists or expert dictionary is rather flexible, there are not definite rules.**\n",
    "\n",
    "3. Create a rule on which you consider the financial stop word should be included or not, afterwards add the final list to the `stop_words` object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example rule:  \n",
    "If appears more times than the average word then exclude it from the list  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "It is possible to update the predifined stop words list from `sklear` by simply giving the list.  \n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop words to add\n",
    "list_stop_words = ['stopword1', 'stopword2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text \n",
    "\n",
    "# Add new stop words\n",
    "new_stop_words = text.ENGLISH_STOP_WORDS.union(list_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Recreate document-term matrix and save the corpus with the NEW list of stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: `cv = CountVectorizer(stop_words=new_stop_words)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Save the following 3 files\n",
    "    - `AnnualReports_cv.pkl` \n",
    "    - `AnnualReports_matrix.pkl` \n",
    "    - `AnnualReports_corpus.pkl` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nteract": {
   "version": "0.15.0"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
